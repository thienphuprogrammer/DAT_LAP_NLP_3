{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Task 5: Transformers Fine-tuning\n\n- Fine-tune TransformerClassifier cho classification\n- Fine-tune TransformerTranslator cho translation\n- Inference TransformerCaptioner cho captioning\n- Đánh giá, visualize kết quả\n"]},
  {"cell_type": "code", "metadata": {}, "source": ["import pandas as pd\nimport numpy as np\nfrom src.data_processor import Flickr8kProcessor\nfrom src.models.transformer_models import TransformerClassifier, TransformerTranslator, TransformerCaptioner\nfrom src.utils.metrics import MetricsCalculator\nfrom transformers import AutoTokenizer\nimport tensorflow as tf\n"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Fine-tune TransformerClassifier cho classification"]},
  {"cell_type": "code", "metadata": {}, "source": ["DATA_PATH = '../data'\nprocessor = Flickr8kProcessor(DATA_PATH)\ncaptions_df = processor.load_captions()\ncaptions_df = processor.create_length_labels(captions_df)\nlabel_map = {'short': 0, 'medium': 1, 'long': 2}\ny = captions_df['length_category'].map(label_map).values\ntexts = captions_df['caption'].tolist()\nclf = TransformerClassifier(model_name='distilbert-base-uncased', num_classes=3)\ntokenizer = clf.tokenizer\nencodings = tokenizer(texts, truncation=True, padding=True, max_length=32)\nX = {k: np.array(v) for k, v in encodings.items()}\ndataset = tf.data.Dataset.from_tensor_slices((X, y)).batch(16)\ntrain_size = int(0.8 * len(texts))\ntrain_dataset = dataset.take(train_size)\nval_dataset = dataset.skip(train_size)\nclf.build_model()\nclf.train(train_dataset, val_dataset, epochs=1)\n"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Fine-tune TransformerTranslator cho translation"]},
  {"cell_type": "code", "metadata": {}, "source": ["df = processor.load_translated_captions('vi')\nsrc_texts = df['caption'].tolist()\ntgt_texts = df['translated_caption'].tolist()\ntranslator = TransformerTranslator(model_name='Helsinki-NLP/opus-mt-vi-en')\ntokenizer = translator.tokenizer\nsrc_enc = tokenizer(src_texts, truncation=True, padding=True, max_length=32, return_tensors='tf')\ntgt_enc = tokenizer(tgt_texts, truncation=True, padding=True, max_length=32, return_tensors='tf')\ntrain_dataset = tf.data.Dataset.from_tensor_slices((dict(src_enc), dict(tgt_enc['input_ids']))).batch(8)\ntranslator.build_model()\ntranslator.train(train_dataset, train_dataset, epochs=1)\n"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Inference TransformerCaptioner cho captioning"]},
  {"cell_type": "code", "metadata": {}, "source": ["captioner = TransformerCaptioner(model_name='nlpconnect/vit-gpt2-image-captioning')\ncaptioner.build_model()\n# caption = captioner.generate_caption('path/to/image.jpg')\n# print('Generated caption:', caption)\n"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}},
 "nbformat": 4,
 "nbformat_minor": 2
}
