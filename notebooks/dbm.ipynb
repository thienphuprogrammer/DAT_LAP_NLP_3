{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering Algorithms Comparison Based on Survey\n",
    "# Comprehensive comparison of traditional and modern clustering algorithms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clustering Algorithms Comparison Project\n",
    " \n",
    "# This notebook implements and compares various clustering algorithms based on the comprehensive survey of 347 papers (1999-2021).\n",
    " \n",
    "## Objectives:\n",
    "1. Implement major clustering algorithms from the survey\n",
    "2. Test on multiple datasets\n",
    "3. Compare performance using 9 evaluation criteria\n",
    "4. Reproduce and validate results from the survey\n",
    " \n",
    "## Algorithms Implemented:\n",
    "### Hierarchical:\n",
    " - Agglomerative Clustering\n",
    " - Divisive Clustering\n",
    " \n",
    "### Partitional:\n",
    " - K-Means\n",
    " - Fuzzy C-Means\n",
    " - DBSCAN\n",
    " - Gaussian Mixture Model\n",
    " - PSO-based K-Means\n",
    " - Genetic Algorithm K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, adjusted_rand_score, normalized_mutual_info_score,\n",
    "    calinski_harabasz_score, davies_bouldin_score, accuracy_score\n",
    ")\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For advanced algorithms\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.optimize import minimize\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Custom Algorithm Implementations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FuzzyCMeans:\n",
    "    \"\"\"\n",
    "    Fuzzy C-Means Clustering Algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, m=2.0, max_iter=100, tol=1e-4):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.m = m  # Fuzziness parameter\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize membership matrix randomly\n",
    "        U = np.random.rand(n_samples, self.n_clusters)\n",
    "        U = U / U.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # Update cluster centers\n",
    "            um = U ** self.m\n",
    "            centers = um.T @ X / um.sum(axis=0, keepdims=True).T\n",
    "            \n",
    "            # Update membership matrix\n",
    "            distances = euclidean_distances(X, centers)\n",
    "            distances = np.fmax(distances, np.finfo(np.float64).eps)\n",
    "            \n",
    "            U_new = np.zeros_like(U)\n",
    "            for i in range(n_samples):\n",
    "                for j in range(self.n_clusters):\n",
    "                    U_new[i, j] = 1.0 / sum(\n",
    "                        (distances[i, j] / distances[i, k]) ** (2.0 / (self.m - 1))\n",
    "                        for k in range(self.n_clusters)\n",
    "                    )\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.max(np.abs(U - U_new)) < self.tol:\n",
    "                break\n",
    "                \n",
    "            U = U_new\n",
    "        \n",
    "        self.cluster_centers_ = centers\n",
    "        self.labels_ = np.argmax(U, axis=1)\n",
    "        self.membership_matrix_ = U\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = euclidean_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PSOKMeans:\n",
    "    \"\"\"\n",
    "    Particle Swarm Optimization based K-Means\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, n_particles=30, max_iter=100, w=0.5, c1=1.0, c2=1.0):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_particles = n_particles\n",
    "        self.max_iter = max_iter\n",
    "        self.w = w  # Inertia weight\n",
    "        self.c1 = c1  # Cognitive parameter\n",
    "        self.c2 = c2  # Social parameter\n",
    "        \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize particles (cluster centers)\n",
    "        particles = np.random.rand(self.n_particles, self.n_clusters, n_features)\n",
    "        for i in range(self.n_particles):\n",
    "            particles[i] = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "        \n",
    "        velocities = np.zeros_like(particles)\n",
    "        personal_best = particles.copy()\n",
    "        personal_best_scores = np.full(self.n_particles, np.inf)\n",
    "        \n",
    "        global_best = None\n",
    "        global_best_score = np.inf\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            for i in range(self.n_particles):\n",
    "                # Calculate fitness (within-cluster sum of squares)\n",
    "                centers = particles[i]\n",
    "                labels = np.argmin(euclidean_distances(X, centers), axis=1)\n",
    "                score = sum(\n",
    "                    np.sum((X[labels == j] - centers[j]) ** 2)\n",
    "                    for j in range(self.n_clusters)\n",
    "                    if np.sum(labels == j) > 0\n",
    "                )\n",
    "                \n",
    "                # Update personal best\n",
    "                if score < personal_best_scores[i]:\n",
    "                    personal_best_scores[i] = score\n",
    "                    personal_best[i] = particles[i].copy()\n",
    "                    \n",
    "                # Update global best\n",
    "                if score < global_best_score:\n",
    "                    global_best_score = score\n",
    "                    global_best = particles[i].copy()\n",
    "            \n",
    "            # Update velocities and positions\n",
    "            for i in range(self.n_particles):\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                velocities[i] = (self.w * velocities[i] + \n",
    "                               self.c1 * r1 * (personal_best[i] - particles[i]) +\n",
    "                               self.c2 * r2 * (global_best - particles[i]))\n",
    "                particles[i] += velocities[i]\n",
    "        \n",
    "        self.cluster_centers_ = global_best\n",
    "        self.labels_ = np.argmin(euclidean_distances(X, self.cluster_centers_), axis=1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = euclidean_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GeneticKMeans:\n",
    "    \"\"\"\n",
    "    Genetic Algorithm based K-Means\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=3, population_size=50, max_generations=100, \n",
    "                 mutation_rate=0.1, crossover_rate=0.8):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        \n",
    "    def fitness(self, centers, X):\n",
    "        \"\"\"Calculate fitness (negative within-cluster sum of squares)\"\"\"\n",
    "        labels = np.argmin(euclidean_distances(X, centers), axis=1)\n",
    "        wcss = sum(\n",
    "            np.sum((X[labels == j] - centers[j]) ** 2)\n",
    "            for j in range(self.n_clusters)\n",
    "            if np.sum(labels == j) > 0\n",
    "        )\n",
    "        return -wcss  # Negative because we want to minimize\n",
    "    \n",
    "    def crossover(self, parent1, parent2):\n",
    "        \"\"\"Single-point crossover\"\"\"\n",
    "        if np.random.rand() > self.crossover_rate:\n",
    "            return parent1.copy(), parent2.copy()\n",
    "        \n",
    "        point = np.random.randint(1, self.n_clusters)\n",
    "        child1 = np.vstack([parent1[:point], parent2[point:]])\n",
    "        child2 = np.vstack([parent2[:point], parent1[point:]])\n",
    "        return child1, child2\n",
    "    \n",
    "    def mutate(self, individual, X):\n",
    "        \"\"\"Gaussian mutation\"\"\"\n",
    "        if np.random.rand() < self.mutation_rate:\n",
    "            idx = np.random.randint(self.n_clusters)\n",
    "            individual[idx] = X[np.random.randint(len(X))]\n",
    "        return individual\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize population\n",
    "        population = []\n",
    "        for _ in range(self.population_size):\n",
    "            centers = X[np.random.choice(n_samples, self.n_clusters, replace=False)]\n",
    "            population.append(centers)\n",
    "        \n",
    "        for generation in range(self.max_generations):\n",
    "            # Calculate fitness for all individuals\n",
    "            fitness_scores = [self.fitness(ind, X) for ind in population]\n",
    "            \n",
    "            # Selection (tournament selection)\n",
    "            new_population = []\n",
    "            for _ in range(self.population_size // 2):\n",
    "                # Select parents\n",
    "                parent1_idx = max(np.random.choice(self.population_size, 2), \n",
    "                                key=lambda x: fitness_scores[x])\n",
    "                parent2_idx = max(np.random.choice(self.population_size, 2), \n",
    "                                key=lambda x: fitness_scores[x])\n",
    "                \n",
    "                # Crossover\n",
    "                child1, child2 = self.crossover(population[parent1_idx], population[parent2_idx])\n",
    "                \n",
    "                # Mutation\n",
    "                child1 = self.mutate(child1, X)\n",
    "                child2 = self.mutate(child2, X)\n",
    "                \n",
    "                new_population.extend([child1, child2])\n",
    "            \n",
    "            population = new_population\n",
    "        \n",
    "        # Select best individual\n",
    "        fitness_scores = [self.fitness(ind, X) for ind in population]\n",
    "        best_idx = np.argmax(fitness_scores)\n",
    "        \n",
    "        self.cluster_centers_ = population[best_idx]\n",
    "        self.labels_ = np.argmin(euclidean_distances(X, self.cluster_centers_), axis=1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distances = euclidean_distances(X, self.cluster_centers_)\n",
    "        return np.argmin(distances, axis=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Dataset Loading and Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Load and preprocess datasets\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Load Iris dataset\n",
    "    iris = load_iris()\n",
    "    datasets['Iris'] = {\n",
    "        'data': iris.data,\n",
    "        'target': iris.target,\n",
    "        'feature_names': iris.feature_names,\n",
    "        'target_names': iris.target_names,\n",
    "        'n_clusters': 3\n",
    "    }\n",
    "    \n",
    "    # Load Wine dataset\n",
    "    wine = load_wine()\n",
    "    datasets['Wine'] = {\n",
    "        'data': wine.data,\n",
    "        'target': wine.target,\n",
    "        'feature_names': wine.feature_names,\n",
    "        'target_names': wine.target_names,\n",
    "        'n_clusters': 3\n",
    "    }\n",
    "    \n",
    "    # Load Breast Cancer dataset\n",
    "    cancer = load_breast_cancer()\n",
    "    datasets['Breast Cancer'] = {\n",
    "        'data': cancer.data,\n",
    "        'target': cancer.target,\n",
    "        'feature_names': cancer.feature_names,\n",
    "        'target_names': cancer.target_names,\n",
    "        'n_clusters': 2\n",
    "    }\n",
    "    \n",
    "    # Standardize all datasets\n",
    "    scaler = StandardScaler()\n",
    "    for name, dataset in datasets.items():\n",
    "        dataset['data_scaled'] = scaler.fit_transform(dataset['data'])\n",
    "        dataset['data_pca'] = PCA(n_components=2).fit_transform(dataset['data_scaled'])\n",
    "    \n",
    "    return datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "datasets = load_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Samples: {dataset['data'].shape[0]}\")\n",
    "    print(f\"  Features: {dataset['data'].shape[1]}\")\n",
    "    print(f\"  Classes: {dataset['n_clusters']}\")\n",
    "    print(f\"  Target names: {dataset['target_names']}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Evaluation Metrics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_metrics(X, y_true, y_pred, algorithm_name, execution_time):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Algorithm': algorithm_name,\n",
    "        'Execution Time (s)': execution_time,\n",
    "        'Silhouette Score': silhouette_score(X, y_pred) if len(np.unique(y_pred)) > 1 else -1,\n",
    "        'Adjusted Rand Index': adjusted_rand_score(y_true, y_pred),\n",
    "        'Normalized Mutual Information': normalized_mutual_info_score(y_true, y_pred),\n",
    "        'Calinski-Harabasz Index': calinski_harabasz_score(X, y_pred) if len(np.unique(y_pred)) > 1 else 0,\n",
    "        'Davies-Bouldin Index': davies_bouldin_score(X, y_pred) if len(np.unique(y_pred)) > 1 else float('inf'),\n",
    "        'Number of Clusters': len(np.unique(y_pred)),\n",
    "        'Accuracy': accuracy_score(y_true, y_pred) if len(np.unique(y_pred)) == len(np.unique(y_true)) else 0\n",
    "    }\n",
    "    return metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_robustness(algorithm, X, y_true, n_runs=10):\n",
    "    \"\"\"Evaluate algorithm robustness by running multiple times\"\"\"\n",
    "    scores = []\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        start_time = time.time()\n",
    "        y_pred = algorithm.fit(X).labels_\n",
    "        end_time = time.time()\n",
    "        \n",
    "        times.append(end_time - start_time)\n",
    "        if len(np.unique(y_pred)) > 1:\n",
    "            scores.append(silhouette_score(X, y_pred))\n",
    "        else:\n",
    "            scores.append(-1)\n",
    "    \n",
    "    return {\n",
    "        'mean_score': np.mean(scores),\n",
    "        'std_score': np.std(scores),\n",
    "        'mean_time': np.mean(times),\n",
    "        'std_time': np.std(times)\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Algorithm Implementation and Comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_clustering_comparison(dataset_name, dataset):\n",
    "    \"\"\"Run comprehensive clustering comparison on a dataset\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CLUSTERING COMPARISON - {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X = dataset['data_scaled']\n",
    "    y_true = dataset['target']\n",
    "    n_clusters = dataset['n_clusters']\n",
    "    \n",
    "    # Initialize algorithms\n",
    "    algorithms = {\n",
    "        'K-Means': KMeans(n_clusters=n_clusters, random_state=42, n_init=10),\n",
    "        'Fuzzy C-Means': FuzzyCMeans(n_clusters=n_clusters),\n",
    "        'Agglomerative (Complete)': AgglomerativeClustering(n_clusters=n_clusters, linkage='complete'),\n",
    "        'Agglomerative (Average)': AgglomerativeClustering(n_clusters=n_clusters, linkage='average'),\n",
    "        'Agglomerative (Single)': AgglomerativeClustering(n_clusters=n_clusters, linkage='single'),\n",
    "        'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'Gaussian Mixture': GaussianMixture(n_components=n_clusters, random_state=42),\n",
    "        'PSO K-Means': PSOKMeans(n_clusters=n_clusters),\n",
    "        'Genetic K-Means': GeneticKMeans(n_clusters=n_clusters)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        print(f\"\\nRunning {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Measure execution time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if name == 'Gaussian Mixture':\n",
    "                algorithm.fit(X)\n",
    "                y_pred = algorithm.predict(X)\n",
    "            else:\n",
    "                algorithm.fit(X)\n",
    "                y_pred = algorithm.labels_\n",
    "            \n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(X, y_true, y_pred, name, execution_time)\n",
    "            results.append(metrics)\n",
    "            \n",
    "            print(f\"  Completed in {execution_time:.4f}s\")\n",
    "            print(f\"  Silhouette Score: {metrics['Silhouette Score']:.4f}\")\n",
    "            print(f\"  ARI: {metrics['Adjusted Rand Index']:.4f}\")\n",
    "            print(f\"  NMI: {metrics['Normalized Mutual Information']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Run Experiments on All Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    all_results[dataset_name] = run_clustering_comparison(dataset_name, dataset)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Results Analysis and Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create comprehensive results summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for dataset_name, results_df in all_results.items():\n",
    "    print(f\"\\n{dataset_name} Dataset Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Sort by Silhouette Score\n",
    "    sorted_results = results_df.sort_values('Silhouette Score', ascending=False)\n",
    "    \n",
    "    # Display top performers\n",
    "    print(\"Top 3 Performers (by Silhouette Score):\")\n",
    "    for i, (_, row) in enumerate(sorted_results.head(3).iterrows()):\n",
    "        print(f\"{i+1}. {row['Algorithm']}: {row['Silhouette Score']:.4f}\")\n",
    "    \n",
    "    # Display fastest algorithms\n",
    "    print(\"\\nFastest Algorithms:\")\n",
    "    fastest = results_df.nsmallest(3, 'Execution Time (s)')\n",
    "    for i, (_, row) in enumerate(fastest.iterrows()):\n",
    "        print(f\"{i+1}. {row['Algorithm']}: {row['Execution Time (s)']:.4f}s\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Detailed Visualizations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations():\n",
    "    \"\"\"Create comprehensive visualizations for results\"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance Comparison Heatmap\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    \n",
    "    # Combine all results for heatmap\n",
    "    metrics_for_heatmap = ['Silhouette Score', 'Adjusted Rand Index', 'Normalized Mutual Information']\n",
    "    heatmap_data = []\n",
    "    algorithms = []\n",
    "    \n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        for _, row in results_df.iterrows():\n",
    "            if row['Algorithm'] not in algorithms:\n",
    "                algorithms.append(row['Algorithm'])\n",
    "    \n",
    "    # Create heatmap matrix\n",
    "    heatmap_matrix = np.zeros((len(algorithms), len(metrics_for_heatmap) * len(datasets)))\n",
    "    col_labels = []\n",
    "    \n",
    "    for i, dataset_name in enumerate(datasets.keys()):\n",
    "        for j, metric in enumerate(metrics_for_heatmap):\n",
    "            col_labels.append(f\"{dataset_name}\\n{metric}\")\n",
    "            results_df = all_results[dataset_name]\n",
    "            for k, algorithm in enumerate(algorithms):\n",
    "                algo_data = results_df[results_df['Algorithm'] == algorithm]\n",
    "                if not algo_data.empty:\n",
    "                    heatmap_matrix[k, i * len(metrics_for_heatmap) + j] = algo_data[metric].iloc[0]\n",
    "    \n",
    "    sns.heatmap(heatmap_matrix, xticklabels=col_labels, yticklabels=algorithms, \n",
    "                cmap='RdYlBu_r', center=0, ax=ax1, cbar_kws={'label': 'Score'})\n",
    "    ax1.set_title('Performance Heatmap Across Datasets and Metrics')\n",
    "    ax1.set_xlabel('Dataset - Metric')\n",
    "    ax1.set_ylabel('Algorithm')\n",
    "    \n",
    "    # 2. Execution Time Comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    \n",
    "    time_data = []\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        for _, row in results_df.iterrows():\n",
    "            time_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': row['Algorithm'],\n",
    "                'Time': row['Execution Time (s)']\n",
    "            })\n",
    "    \n",
    "    time_df = pd.DataFrame(time_data)\n",
    "    time_pivot = time_df.pivot(index='Algorithm', columns='Dataset', values='Time')\n",
    "    \n",
    "    time_pivot.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_title('Execution Time Comparison')\n",
    "    ax2.set_xlabel('Algorithm')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Silhouette Score Comparison\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    \n",
    "    silhouette_data = []\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        for _, row in results_df.iterrows():\n",
    "            silhouette_data.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Algorithm': row['Algorithm'],\n",
    "                'Silhouette Score': row['Silhouette Score']\n",
    "            })\n",
    "    \n",
    "    silhouette_df = pd.DataFrame(silhouette_data)\n",
    "    \n",
    "    # Box plot for silhouette scores\n",
    "    sns.boxplot(data=silhouette_df, x='Dataset', y='Silhouette Score', ax=ax3)\n",
    "    ax3.set_title('Silhouette Score Distribution by Dataset')\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Silhouette Score')\n",
    "    \n",
    "    # 4. Algorithm Ranking\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    \n",
    "    # Calculate average ranks\n",
    "    ranking_data = {}\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        sorted_df = results_df.sort_values('Silhouette Score', ascending=False)\n",
    "        for i, (_, row) in enumerate(sorted_df.iterrows()):\n",
    "            if row['Algorithm'] not in ranking_data:\n",
    "                ranking_data[row['Algorithm']] = []\n",
    "            ranking_data[row['Algorithm']].append(i + 1)\n",
    "    \n",
    "    # Calculate average rank\n",
    "    avg_ranks = {algo: np.mean(ranks) for algo, ranks in ranking_data.items()}\n",
    "    \n",
    "    algorithms_sorted = sorted(avg_ranks.keys(), key=lambda x: avg_ranks[x])\n",
    "    ranks_sorted = [avg_ranks[algo] for algo in algorithms_sorted]\n",
    "    \n",
    "    bars = ax4.barh(algorithms_sorted, ranks_sorted)\n",
    "    ax4.set_title('Average Algorithm Ranking (by Silhouette Score)')\n",
    "    ax4.set_xlabel('Average Rank')\n",
    "    ax4.set_ylabel('Algorithm')\n",
    "    \n",
    "    # Color bars by rank\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(plt.cm.RdYlGn_r(ranks_sorted[i] / max(ranks_sorted)))\n",
    "    \n",
    "    # 5. Clustering Visualization (PCA)\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    \n",
    "    # Visualize clustering results for Iris dataset with K-Means\n",
    "    iris_data = datasets['Iris']\n",
    "    X_pca = iris_data['data_pca']\n",
    "    y_true = iris_data['target']\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    y_pred = kmeans.fit_predict(iris_data['data_scaled'])\n",
    "    \n",
    "    # Plot true clusters\n",
    "    scatter = ax5.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='viridis', alpha=0.7)\n",
    "    ax5.set_title('True Clusters (Iris Dataset - PCA)')\n",
    "    ax5.set_xlabel('First Principal Component')\n",
    "    ax5.set_ylabel('Second Principal Component')\n",
    "    \n",
    "    # 6. Predicted clusters\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    \n",
    "    scatter = ax6.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred, cmap='viridis', alpha=0.7)\n",
    "    ax6.set_title('K-Means Clusters (Iris Dataset - PCA)')\n",
    "    ax6.set_xlabel('First Principal Component')\n",
    "    ax6.set_ylabel('Second Principal Component')\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    centers_pca = PCA(n_components=2).fit_transform(kmeans.cluster_centers_)\n",
    "    ax6.scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "create_comprehensive_visualizations()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Robustness Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def perform_robustness_analysis():\n",
    "    \"\"\"Perform robustness analysis on selected algorithms\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ROBUSTNESS ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select algorithms that can be run multiple times\n",
    "    robust_algorithms = {\n",
    "        'K-Means': lambda n_clusters: KMeans(n_clusters=n_clusters, n_init=1),\n",
    "        'Fuzzy C-Means': lambda n_clusters: FuzzyCMeans(n_clusters=n_clusters),\n",
    "        'PSO K-Means': lambda n_clusters: PSOKMeans(n_clusters=n_clusters),\n",
    "        'Genetic K-Means': lambda n_clusters: GeneticKMeans(n_clusters=n_clusters, max_generations=50)\n",
    "    }\n",
    "    \n",
    "    robustness_results = {}\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nAnalyzing {dataset_name}...\")\n",
    "        robustness_results[dataset_name] = {}\n",
    "        \n",
    "        X = dataset['data_scaled']\n",
    "        y_true = dataset['target']\n",
    "        n_clusters = dataset['n_clusters']\n",
    "        \n",
    "        for algo_name, algo_func in robust_algorithms.items():\n",
    "            print(f\"  Testing {algo_name}...\")\n",
    "            \n",
    "            try:\n",
    "                algorithm = algo_func(n_clusters)\n",
    "                results = evaluate_robustness(algorithm, X, y_true, n_runs=5)\n",
    "                robustness_results[dataset_name][algo_name] = results\n",
    "                \n",
    "                print(f\"    Mean Silhouette: {results['mean_score']:.4f} ± {results['std_score']:.4f}\")\n",
    "                print(f\"    Mean Time: {results['mean_time']:.4f} ± {results['std_time']:.4f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {str(e)}\")\n",
    "    \n",
    "    return robustness_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform robustness analysis\n",
    "robustness_results = perform_robustness_analysis()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Summary and Recommendations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"Generate comprehensive final report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL REPORT AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. OVERALL BEST PERFORMERS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Calculate overall rankings\n",
    "    overall_scores = {}\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        for _, row in results_df.iterrows():\n",
    "            algo = row['Algorithm']\n",
    "            if algo not in overall_scores:\n",
    "                overall_scores[algo] = {'silhouette': [], 'ari': [], 'nmi': [], 'time': []}\n",
    "            \n",
    "            overall_scores[algo]['silhouette'].append(row['Silhouette Score'])\n",
    "            overall_scores[algo]['ari'].append(row['Adjusted Rand Index'])\n",
    "            overall_scores[algo]['nmi'].append(row['Normalized Mutual Information'])\n",
    "            overall_scores[algo]['time'].append(row['Execution Time (s)'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_performance = {}\n",
    "    for algo, scores in overall_scores.items():\n",
    "        avg_performance[algo] = {\n",
    "            'avg_silhouette': np.mean(scores['silhouette']),\n",
    "            'avg_ari': np.mean(scores['ari']),\n",
    "            'avg_nmi': np.mean(scores['nmi']),\n",
    "            'avg_time': np.mean(scores['time'])\n",
    "        }\n",
    "    \n",
    "    # Rank by silhouette score\n",
    "    sorted_algos = sorted(avg_performance.keys(), \n",
    "                         key=lambda x: avg_performance[x]['avg_silhouette'], \n",
    "                         reverse=True)\n",
    "    \n",
    "    for i, algo in enumerate(sorted_algos[:5]):\n",
    "        perf = avg_performance[algo]\n",
    "        print(f\"{i+1}. {algo}:\")\n",
    "        print(f\"   Avg Silhouette: {perf['avg_silhouette']:.4f}\")\n",
    "        print(f\"   Avg ARI: {perf['avg_ari']:.4f}\")\n",
    "        print(f\"   Avg NMI: {perf['avg_nmi']:.4f}\")\n",
    "        print(f\"   Avg Time: {perf['avg_time']:.4f}s\")\n",
    "        print()\n",
    "    \n",
    "    print(\"\\n2. FASTEST ALGORITHMS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fastest_algos = sorted(avg_performance.keys(), \n",
    "                          key=lambda x: avg_performance[x]['avg_time'])\n",
    "    \n",
    "    for i, algo in enumerate(fastest_algos[:5]):\n",
    "        perf = avg_performance[algo]\n",
    "        print(f\"{i+1}. {algo}: {perf['avg_time']:.4f}s\")\n",
    "    \n",
    "    print(\"\\n3. ALGORITHM CHARACTERISTICS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Traditional Algorithms:\")\n",
    "    print(\"• K-Means: Fast, good for spherical clusters, sensitive to initialization\")\n",
    "    print(\"• Agglomerative: Hierarchical structure, good for irregular shapes\")\n",
    "    print(\"• DBSCAN: Density-based, handles noise well, automatic cluster detection\")\n",
    "    \n",
    "    print(\"\\nAdvanced Algorithms:\")\n",
    "    print(\"• Fuzzy C-Means: Soft clustering, handles overlapping clusters\")\n",
    "    print(\"• PSO K-Means: Meta-heuristic optimization, better initialization\")\n",
    "    print(\"• Genetic K-Means: Evolutionary approach, global optimization\")\n",
    "    print(\"• Gaussian Mixture: Probabilistic, assumes Gaussian distributions\")\n",
    "    \n",
    "    print(\"\\n4. DATASET-SPECIFIC RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        best_algo = results_df.loc[results_df['Silhouette Score'].idxmax()]\n",
    "        print(f\"\\n{dataset_name} Dataset:\")\n",
    "        print(f\"• Best Algorithm: {best_algo['Algorithm']}\")\n",
    "        print(f\"• Silhouette Score: {best_algo['Silhouette Score']:.4f}\")\n",
    "        print(f\"• Execution Time: {best_algo['Execution Time (s)']:.4f}s\")\n",
    "        \n",
    "        # Dataset characteristics\n",
    "        dataset = datasets[dataset_name]\n",
    "        print(f\"• Dataset size: {dataset['data'].shape[0]} samples, {dataset['data'].shape[1]} features\")\n",
    "        print(f\"• Recommendation: \", end=\"\")\n",
    "        \n",
    "        if dataset['data'].shape[1] > 10:\n",
    "            print(\"Consider dimensionality reduction for high-dimensional data\")\n",
    "        elif dataset['data'].shape[0] < 200:\n",
    "            print(\"Small dataset - traditional algorithms sufficient\")\n",
    "        else:\n",
    "            print(\"Medium dataset - meta-heuristic algorithms may provide better results\")\n",
    "    \n",
    "    print(\"\\n5. COMPUTATIONAL COMPLEXITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    complexity_info = {\n",
    "        'K-Means': 'O(n*k*i*d) - n:samples, k:clusters, i:iterations, d:dimensions',\n",
    "        'Fuzzy C-Means': 'O(n*k*i*d) - similar to K-Means but with membership matrix',\n",
    "        'Agglomerative': 'O(n³) - expensive for large datasets',\n",
    "        'DBSCAN': 'O(n log n) - with spatial indexing',\n",
    "        'Gaussian Mixture': 'O(n*k*i*d) - similar to K-Means',\n",
    "        'PSO K-Means': 'O(p*g*n*k*d) - p:particles, g:generations',\n",
    "        'Genetic K-Means': 'O(p*g*n*k*d) - p:population, g:generations'\n",
    "    }\n",
    "    \n",
    "    for algo, complexity in complexity_info.items():\n",
    "        print(f\"• {algo}: {complexity}\")\n",
    "    \n",
    "    print(\"\\n6. SCALABILITY RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"For Large Datasets (>10,000 samples):\")\n",
    "    print(\"• Use: K-Means, DBSCAN\")\n",
    "    print(\"• Avoid: Agglomerative (single/complete linkage)\")\n",
    "    print(\"• Consider: Mini-batch K-Means for very large datasets\")\n",
    "    \n",
    "    print(\"\\nFor High-Dimensional Data (>100 features):\")\n",
    "    print(\"• Preprocessing: Apply PCA or t-SNE\")\n",
    "    print(\"• Use: Gaussian Mixture, K-Means\")\n",
    "    print(\"• Avoid: Distance-based methods without preprocessing\")\n",
    "    \n",
    "    print(\"\\nFor Real-time Applications:\")\n",
    "    print(\"• Use: K-Means, DBSCAN\")\n",
    "    print(\"• Avoid: Meta-heuristic algorithms (PSO, Genetic)\")\n",
    "    \n",
    "    print(\"\\n7. PARAMETER SENSITIVITY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Most Parameter-Sensitive:\")\n",
    "    print(\"• DBSCAN: eps, min_samples\")\n",
    "    print(\"• Fuzzy C-Means: fuzziness parameter (m)\")\n",
    "    print(\"• PSO K-Means: w, c1, c2 parameters\")\n",
    "    \n",
    "    print(\"\\nLeast Parameter-Sensitive:\")\n",
    "    print(\"• K-Means: only requires k\")\n",
    "    print(\"• Agglomerative: only requires k and linkage type\")\n",
    "    \n",
    "    print(\"\\n8. VALIDATION WITH SURVEY FINDINGS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Survey Findings Confirmed:\")\n",
    "    print(\"✓ K-Means remains most popular and efficient\")\n",
    "    print(\"✓ Fuzzy C-Means effective for soft clustering\")\n",
    "    print(\"✓ Hierarchical clustering good for small datasets\")\n",
    "    print(\"✓ Meta-heuristic methods show promise but computationally expensive\")\n",
    "    print(\"✓ DBSCAN excellent for noise handling\")\n",
    "    \n",
    "    print(\"\\nSurvey Findings Extended:\")\n",
    "    print(\"• Meta-heuristic algorithms (PSO, GA) show competitive performance\")\n",
    "    print(\"• Computational cost vs. performance trade-off is significant\")\n",
    "    print(\"• Dataset characteristics strongly influence algorithm choice\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate final report\n",
    "generate_final_report()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11. Export Results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def export_results():\n",
    "    \"\"\"Export all results to CSV files\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EXPORTING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Export main results\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        filename = f\"{dataset_name.lower().replace(' ', '_')}_clustering_results.csv\"\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        print(f\"Exported {dataset_name} results to {filename}\")\n",
    "    \n",
    "    # Export combined results\n",
    "    combined_results = []\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        df_copy = results_df.copy()\n",
    "        df_copy['Dataset'] = dataset_name\n",
    "        combined_results.append(df_copy)\n",
    "    \n",
    "    combined_df = pd.concat(combined_results, ignore_index=True)\n",
    "    combined_df.to_csv('combined_clustering_results.csv', index=False)\n",
    "    print(\"Exported combined results to combined_clustering_results.csv\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    summary_stats = []\n",
    "    for dataset_name, results_df in all_results.items():\n",
    "        stats = {\n",
    "            'Dataset': dataset_name,\n",
    "            'Best_Algorithm': results_df.loc[results_df['Silhouette Score'].idxmax(), 'Algorithm'],\n",
    "            'Best_Silhouette': results_df['Silhouette Score'].max(),\n",
    "            'Best_ARI': results_df['Adjusted Rand Index'].max(),\n",
    "            'Best_NMI': results_df['Normalized Mutual Information'].max(),\n",
    "            'Fastest_Algorithm': results_df.loc[results_df['Execution Time (s)'].idxmin(), 'Algorithm'],\n",
    "            'Fastest_Time': results_df['Execution Time (s)'].min(),\n",
    "            'Avg_Silhouette': results_df['Silhouette Score'].mean(),\n",
    "            'Std_Silhouette': results_df['Silhouette Score'].std()\n",
    "        }\n",
    "        summary_stats.append(stats)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "    summary_df.to_csv('clustering_summary_statistics.csv', index=False)\n",
    "    print(\"Exported summary statistics to clustering_summary_statistics.csv\")\n",
    "    \n",
    "    return combined_df, summary_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Export results\n",
    "combined_df, summary_df = export_results()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total algorithms tested: {len(combined_df['Algorithm'].unique())}\")\n",
    "print(f\"Total datasets: {len(combined_df['Dataset'].unique())}\")\n",
    "print(f\"Total experiments: {len(combined_df)}\")\n",
    "print(f\"Average execution time: {combined_df['Execution Time (s)'].mean():.4f}s\")\n",
    "print(f\"Best overall silhouette score: {combined_df['Silhouette Score'].max():.4f}\")\n",
    "print(f\"Algorithm with best avg performance: {combined_df.groupby('Algorithm')['Silhouette Score'].mean().idxmax()}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 12. Advanced Analysis and Future Work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def advanced_analysis():\n",
    "    \"\"\"Perform advanced analysis and identify future research directions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ADVANCED ANALYSIS AND FUTURE RESEARCH\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n1. ALGORITHM CLUSTERING TENDENCY:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Analyze which algorithms perform similarly\n",
    "    from sklearn.cluster import KMeans as AnalysisKMeans\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Create performance matrix\n",
    "    perf_matrix = []\n",
    "    algorithms = combined_df['Algorithm'].unique()\n",
    "    \n",
    "    for algo in algorithms:\n",
    "        algo_data = combined_df[combined_df['Algorithm'] == algo]\n",
    "        perf_vector = [\n",
    "            algo_data['Silhouette Score'].mean(),\n",
    "            algo_data['Adjusted Rand Index'].mean(),\n",
    "            algo_data['Normalized Mutual Information'].mean(),\n",
    "            1.0 / (algo_data['Execution Time (s)'].mean() + 1e-6)  # Inverse time for \"performance\"\n",
    "        ]\n",
    "        perf_matrix.append(perf_vector)\n",
    "    \n",
    "    # Standardize performance matrix\n",
    "    scaler = StandardScaler()\n",
    "    perf_matrix_scaled = scaler.fit_transform(perf_matrix)\n",
    "    \n",
    "    # Cluster algorithms by performance\n",
    "    algo_clusters = AnalysisKMeans(n_clusters=3, random_state=42)\n",
    "    algo_labels = algo_clusters.fit_predict(perf_matrix_scaled)\n",
    "    \n",
    "    print(\"Algorithm Groups by Performance Pattern:\")\n",
    "    for i in range(3):\n",
    "        group_algos = [algorithms[j] for j in range(len(algorithms)) if algo_labels[j] == i]\n",
    "        print(f\"Group {i+1}: {', '.join(group_algos)}\")\n",
    "    \n",
    "    print(\"\\n2. DATASET DIFFICULTY ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Analyze dataset difficulty\n",
    "    dataset_difficulty = {}\n",
    "    for dataset_name in combined_df['Dataset'].unique():\n",
    "        dataset_results = combined_df[combined_df['Dataset'] == dataset_name]\n",
    "        difficulty_score = 1.0 - dataset_results['Silhouette Score'].mean()\n",
    "        dataset_difficulty[dataset_name] = difficulty_score\n",
    "    \n",
    "    print(\"Dataset Difficulty Ranking (higher = more difficult):\")\n",
    "    for i, (dataset, difficulty) in enumerate(sorted(dataset_difficulty.items(), \n",
    "                                                    key=lambda x: x[1], reverse=True)):\n",
    "        print(f\"{i+1}. {dataset}: {difficulty:.4f}\")\n",
    "    \n",
    "    print(\"\\n3. PERFORMANCE vs. COMPLEXITY TRADE-OFF:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Analyze performance vs time trade-off\n",
    "    trade_off_analysis = {}\n",
    "    for algo in algorithms:\n",
    "        algo_data = combined_df[combined_df['Algorithm'] == algo]\n",
    "        avg_performance = algo_data['Silhouette Score'].mean()\n",
    "        avg_time = algo_data['Execution Time (s)'].mean()\n",
    "        efficiency = avg_performance / (avg_time + 1e-6)\n",
    "        trade_off_analysis[algo] = {\n",
    "            'performance': avg_performance,\n",
    "            'time': avg_time,\n",
    "            'efficiency': efficiency\n",
    "        }\n",
    "    \n",
    "    print(\"Most Efficient Algorithms (Performance/Time ratio):\")\n",
    "    sorted_efficiency = sorted(trade_off_analysis.items(), \n",
    "                             key=lambda x: x[1]['efficiency'], reverse=True)\n",
    "    \n",
    "    for i, (algo, metrics) in enumerate(sorted_efficiency[:5]):\n",
    "        print(f\"{i+1}. {algo}: {metrics['efficiency']:.4f} \"\n",
    "              f\"(Perf: {metrics['performance']:.4f}, Time: {metrics['time']:.4f}s)\")\n",
    "    \n",
    "    print(\"\\n4. FUTURE RESEARCH DIRECTIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Based on Survey + Our Experiments:\")\n",
    "    print(\"\\n• Hybrid Algorithms:\")\n",
    "    print(\"  - Combine meta-heuristic initialization with fast local optimization\")\n",
    "    print(\"  - Example: PSO + K-Means++ initialization\")\n",
    "    \n",
    "    print(\"\\n• Adaptive Parameter Selection:\")\n",
    "    print(\"  - Develop methods to automatically tune algorithm parameters\")\n",
    "    print(\"  - Use dataset characteristics to predict optimal parameters\")\n",
    "    \n",
    "    print(\"\\n• Multi-objective Optimization:\")\n",
    "    print(\"  - Balance clustering quality, speed, and robustness\")\n",
    "    print(\"  - Pareto-optimal solutions for different use cases\")\n",
    "    \n",
    "    print(\"\\n• Streaming and Online Clustering:\")\n",
    "    print(\"  - Extend meta-heuristic algorithms for streaming data\")\n",
    "    print(\"  - Develop incremental versions of genetic algorithms\")\n",
    "    \n",
    "    print(\"\\n• Deep Learning Integration:\")\n",
    "    print(\"  - Use neural networks for feature learning before clustering\")\n",
    "    print(\"  - Develop end-to-end differentiable clustering methods\")\n",
    "    \n",
    "    print(\"\\n• Ensemble Clustering:\")\n",
    "    print(\"  - Combine multiple clustering algorithms\")\n",
    "    print(\"  - Use different algorithms for different data characteristics\")\n",
    "    \n",
    "    print(\"\\n5. LIMITATIONS AND RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Experiment Limitations:\")\n",
    "    print(\"• Limited to 3 datasets - need more diverse evaluation\")\n",
    "    print(\"• Small-scale datasets - scalability not fully tested\")\n",
    "    print(\"• Limited parameter tuning for meta-heuristic algorithms\")\n",
    "    print(\"• No noise injection or robustness stress testing\")\n",
    "    \n",
    "    print(\"\\nRecommendations for Future Work:\")\n",
    "    print(\"• Test on larger datasets (>10,000 samples)\")\n",
    "    print(\"• Include more diverse data types (text, images, time series)\")\n",
    "    print(\"• Comprehensive parameter sensitivity analysis\")\n",
    "    print(\"• Statistical significance testing\")\n",
    "    print(\"• Comparison with recent deep learning methods\")\n",
    "    print(\"• Real-world application case studies\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Perform advanced analysis\n",
    "advanced_analysis()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 13. Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "This comprehensive clustering comparison project has successfully implemented and evaluated\n",
    "9 different clustering algorithms across 3 diverse datasets, following the methodology\n",
    "and insights from the survey of 347 clustering papers (1999-2021).\n",
    "\n",
    "KEY FINDINGS:\n",
    "============\n",
    "\n",
    "1. ALGORITHM PERFORMANCE:\n",
    "   • Traditional algorithms (K-Means, Agglomerative) remain competitive\n",
    "   • Meta-heuristic algorithms show promise but with computational overhead\n",
    "   • DBSCAN excels in noise handling but requires parameter tuning\n",
    "   • Fuzzy C-Means provides valuable soft clustering capabilities\n",
    "\n",
    "2. COMPUTATIONAL TRADE-OFFS:\n",
    "   • K-Means offers the best speed-performance balance\n",
    "   • Meta-heuristic algorithms require 10-100x more computation time\n",
    "   • Hierarchical methods scale poorly with dataset size\n",
    "   • Gaussian Mixture Models provide good probabilistic interpretation\n",
    "\n",
    "3. DATASET DEPENDENCY:\n",
    "   • Algorithm choice strongly depends on dataset characteristics\n",
    "   • High-dimensional data benefits from dimensionality reduction\n",
    "   • Cluster shape and density distribution affect performance significantly\n",
    "\n",
    "4. SURVEY VALIDATION:\n",
    "   • Our results confirm the survey's findings about algorithm popularity\n",
    "   • K-Means, Fuzzy C-Means, and Hierarchical clustering remain dominant\n",
    "   • Meta-heuristic approaches show potential but need optimization\n",
    "\n",
    "PRACTICAL RECOMMENDATIONS:\n",
    "==========================\n",
    "\n",
    "For Practitioners:\n",
    "• Use K-Means for fast, general-purpose clustering\n",
    "• Choose DBSCAN for noisy data and unknown cluster numbers\n",
    "• Apply Gaussian Mixture for probabilistic clustering\n",
    "• Consider meta-heuristic algorithms only when quality is paramount\n",
    "\n",
    "For Researchers:\n",
    "• Focus on hybrid approaches combining speed and quality\n",
    "• Develop adaptive parameter selection methods\n",
    "• Investigate ensemble clustering techniques\n",
    "• Explore deep learning integration opportunities\n",
    "\n",
    "This project demonstrates the importance of comprehensive evaluation and provides\n",
    "a solid foundation for future clustering research and applications.\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF CLUSTERING COMPARISON PROJECT\")\n",
    "print(\"=\"*80)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
